{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQl5n2IOuafL"
   },
   "source": [
    "# **New York City Yellow Taxi Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGQVIB4mEFrZ"
   },
   "source": [
    "## Objective\n",
    "In this case study you will be learning exploratory data analysis (EDA) with the help of a dataset on yellow taxi rides in New York City. This will enable you to understand why EDA is an important step in the process of data science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJVMenVllLUL"
   },
   "source": [
    "## **Problem Statement**\n",
    "As an analyst at an upcoming taxi operation in NYC, you are tasked to use the 2023 taxi trip data to uncover insights that could help optimise taxi operations. The goal is to analyse patterns in the data that can inform strategic decisions to improve service efficiency, maximise revenue, and enhance passenger experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OVfUMlHFkZD"
   },
   "source": [
    "## Tasks\n",
    "You need to perform the following steps for successfully completing this assignment:\n",
    "1. Data Loading\n",
    "2. Data Cleaning\n",
    "3. Exploratory Analysis: Bivariate and Multivariate\n",
    "4. Creating Visualisations to Support the Analysis\n",
    "5. Deriving Insights and Stating Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTxV-3GJUhWm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofebI8ITG-Li"
   },
   "source": [
    "**NOTE:** The marks given along with headings and sub-headings are cumulative marks for those particular headings/sub-headings.<br>\n",
    "\n",
    "The actual marks for each task are specified within the tasks themselves.\n",
    "\n",
    "For example, marks given with heading *2* or sub-heading *2.1* are the cumulative marks, for your reference only. <br>\n",
    "\n",
    "The marks you will receive for completing tasks are given with the tasks.\n",
    "\n",
    "Suppose the marks for two tasks are: 3 marks for 2.1.1 and 2 marks for 3.2.2, or\n",
    "* 2.1.1 [3 marks]\n",
    "* 3.2.2 [2 marks]\n",
    "\n",
    "then, you will earn 3 marks for completing task 2.1.1 and 2 marks for completing task 3.2.2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQjht7dUiHt"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eaCZjHIvfuI"
   },
   "source": [
    "## Data Understanding\n",
    "The yellow taxi trip records include fields capturing pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts.\n",
    "\n",
    "The data is stored in Parquet format (*.parquet*). The dataset is from 2009 to 2024. However, for this assignment, we will only be using the data from 2023.\n",
    "\n",
    "The data for each month is present in a different parquet file. You will get twelve files for each of the months in 2023.\n",
    "\n",
    "The data was collected and provided to the NYC Taxi and Limousine Commission (TLC) by technology providers like vendors and taxi hailing apps. <br>\n",
    "\n",
    "You can find the link to the TLC trip records page here: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LI6qC5IDxZU1"
   },
   "source": [
    "###  Data Description\n",
    "You can find the data description here: [Data Dictionary](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FafCzrDuxHg2"
   },
   "source": [
    "**Trip Records**\n",
    "\n",
    "\n",
    "\n",
    "|Field Name       |description |\n",
    "|:----------------|:-----------|\n",
    "| VendorID | A code indicating the TPEP provider that provided the record. <br> 1= Creative Mobile Technologies, LLC; <br> 2= VeriFone Inc. |\n",
    "| tpep_pickup_datetime | The date and time when the meter was engaged.  |\n",
    "| tpep_dropoff_datetime | The date and time when the meter was disengaged.   |\n",
    "| Passenger_count | The number of passengers in the vehicle. <br> This is a driver-entered value. |\n",
    "| Trip_distance | The elapsed trip distance in miles reported by the taximeter. |\n",
    "| PULocationID | TLC Taxi Zone in which the taximeter was engaged |\n",
    "| DOLocationID | TLC Taxi Zone in which the taximeter was disengaged |\n",
    "|RateCodeID |The final rate code in effect at the end of the trip.<br> 1 = Standard rate <br> 2 = JFK <br> 3 = Newark <br>4 = Nassau or Westchester <br>5 = Negotiated fare <br>6 = Group ride |\n",
    "|Store_and_fwd_flag |This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.  <br>Y= store and forward trip <br>N= not a store and forward trip |\n",
    "|Payment_type| A numeric code signifying how the passenger paid for the trip. <br> 1 = Credit card <br>2 = Cash <br>3 = No charge <br>4 = Dispute <br>5 = Unknown <br>6 = Voided trip |\n",
    "|Fare_amount| The time-and-distance fare calculated by the meter. <br>Extra Miscellaneous extras and surcharges.  Currently, this only includes the 0.50 and 1 USD rush hour and overnight charges. |\n",
    "|MTA_tax |0.50 USD MTA tax that is automatically triggered based on the metered rate in use. |\n",
    "|Improvement_surcharge | 0.30 USD improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015. |\n",
    "|Tip_amount |Tip amount – This field is automatically populated for credit card tips. Cash tips are not included. |\n",
    "| Tolls_amount | Total amount of all tolls paid in trip.  |\n",
    "| total_amount | The total amount charged to passengers. Does not include cash tips. |\n",
    "|Congestion_Surcharge |Total amount collected in trip for NYS congestion surcharge. |\n",
    "| Airport_fee | 1.25 USD for pick up only at LaGuardia and John F. Kennedy Airports|\n",
    "\n",
    "Although the amounts of extra charges and taxes applied are specified in the data dictionary, you will see that some cases have different values of these charges in the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mL-FWYFnVEdE"
   },
   "source": [
    "**Taxi Zones**\n",
    "\n",
    "Each of the trip records contains a field corresponding to the location of the pickup or drop-off of the trip, populated by numbers ranging from 1-263.\n",
    "\n",
    "These numbers correspond to taxi zones, which may be downloaded as a table or map/shapefile and matched to the trip records using a join.\n",
    "\n",
    "This is covered in more detail in later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z66W3s51U0gF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kw-WRzBfyS7j"
   },
   "source": [
    "## **1** Data Preparation\n",
    "\n",
    "<font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM2X-s6lycvQ"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "juReqsAzEdW3"
   },
   "outputs": [],
   "source": [
    "# Import warnings\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "3XZjOlJiy1dr"
   },
   "outputs": [],
   "source": [
    "# Import the libraries you will be using for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsH5LNrSgW9q"
   },
   "outputs": [],
   "source": [
    "# Recommended versions\n",
    "# numpy version: 1.26.4\n",
    "# pandas version: 2.2.2\n",
    "# matplotlib version: 3.10.0\n",
    "# seaborn version: 0.13.2\n",
    "\n",
    "# Check versions\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"seaborn version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgHgbPIepaYl"
   },
   "source": [
    "### **1.1** Load the dataset\n",
    "<font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrGluF_gpeHs"
   },
   "source": [
    "You will see twelve files, one for each month.\n",
    "\n",
    "To read parquet files with Pandas, you have to follow a similar syntax as that for CSV files.\n",
    "\n",
    "`df = pd.read_parquet('file.parquet')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kIpIsuSSzCp9"
   },
   "outputs": [],
   "source": [
    "#Try loading one file\n",
    "\n",
    "df = pd.read_parquet('../data_NYC_Taxi/trip_records/2023-1.parquet')\n",
    "print(\"Number of rows:\", len(df))\n",
    "df.info()\n",
    "\n",
    "# rows_9_to_10 = df[\n",
    "#     (df[\"tpep_pickup_datetime\"].dt.hour >= 9) &\n",
    "#     (df[\"tpep_pickup_datetime\"].dt.hour < 10)\n",
    "# ]\n",
    "# print(\"few rows between 9 and 10 AM:\", rows_9_to_10.sort_values(by='tpep_pickup_datetime').head(50).to_string())\n",
    "# print(\"Number of rows between 9 and 10 AM:\", len(rows_9_to_10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh-keWIyqcyr"
   },
   "source": [
    "How many rows are there? Do you think handling such a large number of rows is computationally feasible when we have to combine the data for all twelve months into one?\n",
    "\n",
    "To handle this, we need to sample a fraction of data from each of the files. How to go about that? Think of a way to select only some portion of the data from each month's file that accurately represents the trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHx7lh_3QGmj"
   },
   "source": [
    "#### Sampling the Data\n",
    "> One way is to take a small percentage of entries for pickup in every hour of a date. So, for all the days in a month, we can iterate through the hours and select 5% values randomly from those. Use `tpep_pickup_datetime` for this. Separate date and hour from the datetime values and then for each date, select some fraction of trips for each of the 24 hours.\n",
    "\n",
    "To sample data, you can use the `sample()` method. Follow this syntax:\n",
    "\n",
    "```Python\n",
    "# sampled_data is an empty DF to keep appending sampled data of each hour\n",
    "# hour_data is the DF of entries for an hour 'X' on a date 'Y'\n",
    "\n",
    "sample = hour_data.sample(frac = 0.05, random_state = 42)\n",
    "# sample 0.05 of the hour_data\n",
    "# random_state is just a seed for sampling, you can define it yourself\n",
    "\n",
    "sampled_data = pd.concat([sampled_data, sample]) # adding data for this hour to the DF\n",
    "```\n",
    "\n",
    "This *sampled_data* will contain 5% values selected at random from each hour.\n",
    "\n",
    "Note that the code given above is only the part that will be used for sampling and not the complete code required for sampling and combining the data files.\n",
    "\n",
    "Keep in mind that you sample by date AND hour, not just hour. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zog80nsqvKp"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp24-wpQrlC5"
   },
   "source": [
    "**1.1.1** <font color = red>[5 marks]</font> <br>\n",
    "Figure out how to sample and combine the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRGOnciGOvq0"
   },
   "source": [
    "**Note:** It is not mandatory to use the method specified above. While sampling, you only need to make sure that your sampled data represents the overall data of all the months accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3cpuzPFvP2iC"
   },
   "outputs": [],
   "source": [
    "# Sample the data\n",
    "# It is recommmended to not load all the files at once to avoid memory overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "h3x5bCWmarTO"
   },
   "outputs": [],
   "source": [
    "# from google.colab import \n",
    "#File is read from the local system\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1EXP0PHzPs0"
   },
   "outputs": [],
   "source": [
    "# Take a small percentage of entries from each hour of every date.\n",
    "# Iterating through the monthly data:\n",
    "#   read a month file -> day -> hour: append sampled data -> move to next hour -> move to next day after 24 hours -> move to next month file\n",
    "# Create a single dataframe for the year combining all the monthly data\n",
    "\n",
    "# Select the folder having data files\n",
    "import os\n",
    "\n",
    "# Select the folder having data files\n",
    "os.chdir('/home/sirkumar/IIITB/EDA//data_NYC_Taxi/trip_records')\n",
    "\n",
    "# Create a list of all the twelve files to read\n",
    "file_list = os.listdir()\n",
    "\n",
    "# initialise an empty dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# iterate through the list of files and sample one by one:\n",
    "for file_name in file_list:\n",
    "    try:\n",
    "        # file path for the current file\n",
    "        file_path = os.path.join(os.getcwd(), file_name)\n",
    "\n",
    "        # Reading the current file. Each Parquet file has data for one month only\n",
    "        print(f\"Reading file: {file_name}\")\n",
    "        monthly_df = pd.read_parquet(file_path)\n",
    "       \n",
    "        # We will store the sampled data for the current date in this df by appending the sampled data from each hour to this\n",
    "        # After completing iteration through each date, we will append this data to the final dataframe.\n",
    "        sampled_data = pd.DataFrame()\n",
    "\n",
    "        # Loop through dates and then loop through every hour of each date\n",
    "        unique_dates = monthly_df[\"tpep_pickup_datetime\"].dt.date.unique()\n",
    "        for date in unique_dates: \n",
    "            #For every day\n",
    "            daily_df = monthly_df[monthly_df[\"tpep_pickup_datetime\"].between(f\"{date} 00:00:00\", f\"{date} 23:59:59\")]\n",
    "\n",
    "            # Iterate through each hour of the selected date\n",
    "            for hour in range(24):\n",
    "                hourly_df = daily_df[daily_df[\"tpep_pickup_datetime\"].between(f\"{date} {hour:02d}:00:00\", f\"{date} {hour:02d}:59:59\")]\n",
    "\n",
    "                # Sample 5% of the hourly data randomly\n",
    "                if not hourly_df.empty:\n",
    "                    sampled_hour = hourly_df.sample(frac=0.05, random_state=42)\n",
    "                    \n",
    "                    # add data of this hour to the dataframe\n",
    "                    sampled_data = pd.concat([sampled_data, sampled_hour], ignore_index=True)\n",
    "\n",
    "\n",
    "        # Concatenate the sampled data of all the dates to a single dataframe\n",
    "        df = pd.concat([df, sampled_data], ignore_index=True)\n",
    "        print(f\"After processing file {file_name}, the cumulative dataframe has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sej6pZkzw2AK"
   },
   "source": [
    "After combining the data files into one DataFrame, convert the new DataFrame to a CSV or parquet file and store it to use directly.\n",
    "\n",
    "Ideally, you can try keeping the total entries to around 250,000 to 300,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okaVAsdPxJow"
   },
   "outputs": [],
   "source": [
    "# Store the df in csv/parquet\n",
    "df.to_parquet('sampled_data_2023.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaOS3H9izZ0N"
   },
   "source": [
    "## **2** Data Cleaning\n",
    "<font color = red>[30 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries you will be using for analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y3PKWqhxRA9"
   },
   "source": [
    "Now we can load the new data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOuOL0SDxQHd"
   },
   "outputs": [],
   "source": [
    "# Load the new data file\n",
    "df = pd.read_parquet('/home/sirkumar/IIITB/EDA/data_NYC_Taxi/trip_records/sampled_data_2023.parquet')\n",
    "print(\"Number of rows in the sampled data:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbzmFKyn1780"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FH83U4A49ErC"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZvPSwJx0S3K"
   },
   "source": [
    "#### **2.1** Fixing Columns\n",
    "<font color = red>[10 marks]</font> <br>\n",
    "\n",
    "Fix/drop any columns as you seem necessary in the below sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "358takCd2FiM"
   },
   "source": [
    "**2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Fix the index and drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHHFyZxa2PEM"
   },
   "outputs": [],
   "source": [
    "# Fix the index and drop any columns that are not needed\n",
    "#In next cell airport_fee is dropped as it has null values only\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIFmxvIT2wsn"
   },
   "source": [
    "**2.1.2** <font color = red>[3 marks]</font> <br>\n",
    "There are two airport fee columns. This is possibly an error in naming columns. Let's see whether these can be combined into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmtgnb1x6TrV"
   },
   "outputs": [],
   "source": [
    "# Combine the two airport fee columns\n",
    "print(\"Unique values in Airport_fee column:\", df['Airport_fee'].unique())\n",
    "print(\"Unique values in airport_fee column:\", df['airport_fee'].unique())\n",
    "\n",
    "# Count null/empty values in Airport_fee\n",
    "print(\"\\nNumber of null/empty rows in Airport_fee:\", df['Airport_fee'].isna().sum())\n",
    "print(\"Number of null/empty rows in airport_fee:\", df['airport_fee'].isna().sum())\n",
    "\n",
    "# Since Airport_fee has fewer nulls, fill its NaN values with values from airport_fee\n",
    "df['Airport_fee'] = df['Airport_fee'].fillna(df['airport_fee'])\n",
    "print(\"After filling, number of null/empty rows in Airport_fee:\", df['Airport_fee'].isna().sum())\n",
    "# Drop the airport_fee column\n",
    "df = df.drop(columns=['airport_fee'])\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g4AHG7mOYgP"
   },
   "source": [
    "**2.1.3** <font color = red>[5 marks]</font> <br>\n",
    "Fix columns with negative (monetary) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBsMT2MII1Hv"
   },
   "outputs": [],
   "source": [
    "# check where values of fare amount are negative\n",
    "numeric_cols =['fare_amount', 'total_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge']\n",
    "RateCodeID = {\n",
    "1 : \"Standard rate\",\n",
    "2 : \"JFK\",\n",
    "3 : \"Newark\",\n",
    "4 : \"Nassau or Westchester\",\n",
    "5 : \"Negotiated fare\",\n",
    "6 : \"Group ride\"\n",
    "}\n",
    "\n",
    "Payment_type = {\n",
    "1 : \"Credit card\",\n",
    "2 : \"Cash\",\n",
    "3 : \"No charge\",\n",
    "4 : \"Dispute\",\n",
    "5 : \"Unknown\",\n",
    "6 : \"Voided trip\"\n",
    "}\n",
    "\n",
    "df = df[df['payment_type'] != 4]  # Filtering for 'Dispute' payment type\n",
    "\n",
    "for col in numeric_cols:\n",
    "    print (f\"Checking column: {col}\")\n",
    "    negative_values = df[df[col] < 0]\n",
    "    print(f\"\\n{col} - Found {len(negative_values)} negative values\")\n",
    "\n",
    "    #convert negative values to positive\n",
    "    if len(negative_values) > 0:\n",
    "        print(f\"Converting negative values in column: {col} to positive\")\n",
    "        df.loc[df[col] < 0, col] = df.loc[df[col] < 0, col].abs()\n",
    "        \n",
    "        # Verify if any negative values remain\n",
    "        negative_values = df[df[col] < 0]\n",
    "        print(f\"After conversion, {col} - Found {len(negative_values)} negative values\")\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNKpDtTh8awi"
   },
   "source": [
    "Did you notice something different in the `RatecodeID` column for above records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Ruyh2vaCRNxr"
   },
   "outputs": [],
   "source": [
    "# Find which columns have negative values\n",
    "\n",
    "for col in numeric_cols:\n",
    "    negative_values = df[df[col] < 0]\n",
    "    if not negative_values.empty:\n",
    "        print(f\"Column '{col}' has {len(negative_values)} negative values.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPehZ6PJg3_J"
   },
   "outputs": [],
   "source": [
    "# fix these negative values\n",
    "# convert negative values to absolute values\n",
    "for col in numeric_cols:\n",
    "    if ( df[col] < 0 ).any():\n",
    "        df[col] = df[col].abs()\n",
    "        print(f\"Converted negative values in column '{col}' to absolute values.\")\n",
    "\n",
    "# verify no negative values remain\n",
    "for col in numeric_cols:\n",
    "    negative_values = df[df[col] < 0]\n",
    "    if negative_values.empty:\n",
    "        print(f\"Column '{col}' has no negative values remaining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2hakCCy6wXI"
   },
   "source": [
    "### **2.2** Handling Missing Values\n",
    "<font color = red>[10 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K-QNPDVVhzR"
   },
   "source": [
    "**2.2.1**  <font color = red>[2 marks]</font> <br>\n",
    "Find the proportion of missing values in each column\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfTa9cct6Gec"
   },
   "outputs": [],
   "source": [
    "# Find the proportion of missing values in each column\n",
    "columns = df.columns\n",
    "missing_values_cols = []\n",
    "for col in columns:\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    total_count = len(df[col])\n",
    "    \n",
    "    if missing_count == 0:\n",
    "        print(f\"Column '{col}': No missing values.\")\n",
    "        continue\n",
    "    \n",
    "    missing_percentage = (missing_count / total_count) * 100\n",
    "\n",
    "    print(f\"Column '{col}': Missing Values = {missing_count}, Total cound = {total_count}, Percentage = {missing_percentage:.2f}%\")\n",
    "    missing_values_cols.append((col))\n",
    "\n",
    "print(\"Columns with missing values:\", missing_values_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UdUl6AL_-E_"
   },
   "source": [
    "**2.2.2**  <font color = red>[3 marks]</font> <br>\n",
    "Handling missing values in `passenger_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmNmhr4q8Xys"
   },
   "outputs": [],
   "source": [
    "# Take the mean of the passenger_count column and fill the NaN values with the mean\n",
    "mean = df['passenger_count'].mean()\n",
    "print(\"Before imputation, number of null/empty rows in passenger_count:\", df['passenger_count'].isna().sum())\n",
    "df['passenger_count'] = df['passenger_count'].fillna(round(mean))\n",
    "print(\"After imputation, number of null/empty rows in passenger_count:\", df['passenger_count'].isna().sum())\n",
    "\n",
    "# check negative counts in passenger_count\n",
    "negative_passenger_counts = df[df['passenger_count'] < 0]\n",
    "print(\"\\nNumber of negative values in passenger_count column:\", len(negative_passenger_counts))\n",
    "\n",
    "#check for zero values in passenger_count with non-zero trip distance\n",
    "zero_passenger_counts = df[(df['passenger_count'] == 0) & (df['trip_distance'] > 0)]\n",
    "#remove these rows\n",
    "print(\"\\nNumber of zero passenger count with non-zero trip distance before removal:\", len(zero_passenger_counts))\n",
    "df = df.drop(zero_passenger_counts.index)\n",
    "print(\"Number of zero passenger count with non-zero trip distance after removal:\", len(zero_passenger_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIPCyR6UCw0c"
   },
   "source": [
    "Did you find zeroes in passenger_count? Handle these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUr4fwkjBUTQ"
   },
   "source": [
    "**2.2.3**  <font color = red>[2 marks]</font> <br>\n",
    "Handle missing values in `RatecodeID`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEgFxytzBkrB"
   },
   "outputs": [],
   "source": [
    "# Fix missing values in 'RatecodeID'\n",
    "print(\"Number of null/empty rows in RatecodeID before imputation:\", df['RatecodeID'].isna().sum())\n",
    "# Remove rows with null RatecodeID\n",
    "df = df.dropna(subset=['RatecodeID'])\n",
    "print(\"After dropping, number of null/empty rows in RatecodeID:\", df['RatecodeID'].isna().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TjHXzuODCUW"
   },
   "source": [
    "**2.2.4**  <font color = red>[3 marks]</font> <br>\n",
    "Impute NaN in `congestion_surcharge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqnabUGC3xOA"
   },
   "outputs": [],
   "source": [
    "# handle null values in congestion_surcharge\n",
    "\n",
    "print(\"Number of null/empty rows in congestion_surcharge before imputation:\", df['congestion_surcharge'].isna().sum())\n",
    "# Impute NaN values in 'congestion_surcharge' with 0\n",
    "df['congestion_surcharge'] = df['congestion_surcharge'].fillna(0)\n",
    "print(\"After imputation, number of null/empty rows in congestion_surcharge:\", df['congestion_surcharge'].isna().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FZEdgAgUbPl"
   },
   "source": [
    "Are there missing values in other columns? Did you find NaN values in some other set of columns? Handle those missing values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDLpyqCRXa3K"
   },
   "outputs": [],
   "source": [
    "# Handle any remaining missing values\n",
    "cols = df.columns\n",
    "for col in cols:\n",
    "    print(f\"Column: {col}:\", end='') #Do not inser newline character \n",
    "    missing_count = df[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Handling remaining missing values in column '{col}'\")\n",
    "    else:\n",
    "        print(f\"No missing values\")\n",
    "\n",
    "#Verify no negative values remain in numeric columns\n",
    "cols = numeric_cols + ['passenger_count', 'RatecodeID', 'congestion_surcharge']\n",
    "for col in cols:\n",
    "    negative_values = df[df[col] < 0]\n",
    "    if not negative_values.empty:\n",
    "        print(f\"Column {col}: {len(negative_values)} negative values.\")\n",
    "    else:\n",
    "        print(f\"Column {col}: No negative values.\")\n",
    "\n",
    "#Check RatecodeID is within valid range\n",
    "invalid_ratecodeid = df[~df['RatecodeID'].isin([1, 2, 3, 4, 5, 6])]\n",
    "if invalid_ratecodeid.empty:\n",
    "    print(\"All RatecodeID values are valid.\")\n",
    "else:\n",
    "    print(f\"Number of invalid RatecodeID values: {len(invalid_ratecodeid)}\")\n",
    "    #drop these rows\n",
    "    df = df.drop(invalid_ratecodeid.index)\n",
    "    print(\"After dropping, number of invalid RatecodeID values:\", len(df[~df['RatecodeID'].isin([1, 2, 3, 4, 5, 6])]))\n",
    "\n",
    "#Check payment_type is within valid range\n",
    "invalid_payment_type = df[~df['payment_type'].isin([1, 2, 3, 4, 5, 6])]\n",
    "if invalid_payment_type.empty:\n",
    "    print(\"All payment_type values are valid.\")\n",
    "else:\n",
    "    print(f\"Number of invalid payment_type values: {len(invalid_payment_type)}\")\n",
    "    #drop these rows\n",
    "    df = df.drop(invalid_payment_type.index)\n",
    "    print(\"After dropping, number of invalid payment_type values:\", len(df[~df['payment_type'].isin([1, 2, 3, 4, 5, 6])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jyQyYIpCztl"
   },
   "source": [
    "### **2.3** Handling Outliers\n",
    "<font color = red>[10 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoVwZzuEMTHB"
   },
   "source": [
    "Before we start fixing outliers, let's perform outlier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHK5K6uV8XpU"
   },
   "outputs": [],
   "source": [
    "# Describe the data and check if there are any potential outliers present\n",
    "\n",
    "\n",
    "df.describe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential out of place values in various columns\n",
    "cols_to_check = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'extra', 'tolls_amount']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(cols_to_check, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)\n",
    "    median_val = df[col].median()\n",
    "    plt.axhline(median_val, color='r', linestyle='--', label='Median')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the figure above, we can see that there are potential outliers in fare_amount, tip_amount, total_amount, extra, tolls_amount columns.\n",
    "#We will handle these outliers in the next steps.\n",
    "# Handle outliers using IQR method\n",
    "cols_to_check = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'extra']\n",
    "for col in cols_to_check:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Filter the dataframe to remove outliers\n",
    "    before_rows = len(df)\n",
    "    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    after_rows = len(df)\n",
    "    print(f\"Column '{col}': Removed {before_rows - after_rows} outlier rows.\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XDuLkufyJ2g"
   },
   "source": [
    "**2.3.1**  <font color = red>[10 marks]</font> <br>\n",
    "Based on the above analysis, it seems that some of the outliers are present due to errors in registering the trips. Fix the outliers.\n",
    "\n",
    "Some points you can look for:\n",
    "- Entries where `trip_distance` is nearly 0 and `fare_amount` is more than 300\n",
    "- Entries where `trip_distance` and `fare_amount` are 0 but the pickup and dropoff zones are different (both distance and fare should not be zero for different zones)\n",
    "- Entries where `trip_distance` is more than 250  miles.\n",
    "- Entries where `payment_type` is 0 (there is no payment_type 0 defined in the data dictionary)\n",
    "\n",
    "These are just some suggestions. You can handle outliers in any way you wish, using the insights from above outlier analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-YNHI8tea9c"
   },
   "source": [
    "How will you fix each of these values? Which ones will you drop and which ones will you replace?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap4IfwXO4yZe"
   },
   "source": [
    "First, let us remove 7+ passenger counts as there are very less instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfsOFY5y9-fA"
   },
   "outputs": [],
   "source": [
    "# remove passenger_count > 6\n",
    "print(\"Berfore removing outliers, number of rows:\", len(df))\n",
    "df = df[df['passenger_count'] <= 6]\n",
    "print(\"After removing outliers, number of rows:\", len(df))\n",
    "\n",
    "#check that trip_durations are positive and non-zero and difference between pickup and dropoff times are consistent with trip_duration\n",
    "df['calculated_trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60  # in minutes\n",
    "inconsistent_durations = df[df['calculated_trip_duration'] <= 0]\n",
    "print(\"Number of inconsistent trip durations:\", len(inconsistent_durations))\n",
    "#remove these rows\n",
    "df = df[df['calculated_trip_duration'] > 0]\n",
    "print(\"After removing inconsistent durations, number of rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCUNe3tu8bie"
   },
   "outputs": [],
   "source": [
    "# Continue with outlier handling\n",
    "\n",
    "original_rows_count = len(df)\n",
    "#Trip distance 0 but fare_amount > 0. Remove such rows\n",
    "df = df[~((df['trip_distance'] == 0) & (df['fare_amount'] > 0))]\n",
    "\n",
    "#Trip distance and fare_amount are zero but pick up and dropoff locations are different\n",
    "problematic_rows = df[\n",
    "    (df['trip_distance'] == 0) &\n",
    "    (df['fare_amount'] == 0) &\n",
    "    ((df['PULocationID'] != df['DOLocationID']))\n",
    "]\n",
    "print(\"Number of problematic rows with zero trip distance and fare amount but different locations:\", len(problematic_rows))    \n",
    "if len(problematic_rows) > 0:\n",
    "    df = df.drop(problematic_rows.index)\n",
    "\n",
    "\n",
    "problematic_rows = df[(df['payment_type'] <= 0) | (df['payment_type'] > 6) ]    \n",
    "print(\"Number of problematic rows with invalid payment_type\", len(problematic_rows))\n",
    "\n",
    "if len(problematic_rows) > 0:\n",
    "     df = df.drop(problematic_rows.index)\n",
    "\n",
    "problematic_rows = df[(df['RatecodeID'] <= 0) | (df['RatecodeID'] > 6)]\n",
    "print(\"Number of problematic rows with invalid RatecodeID\", len(problematic_rows))\n",
    "if len(problematic_rows) > 0:\n",
    "    df = df.drop(problematic_rows.index)\n",
    "\n",
    "\n",
    "print(f\"Original rows count: {original_rows_count} rows dropped {original_rows_count- len(df)}, Percentage dropped: {((original_rows_count - len(df)) / original_rows_count) * 100:.2f}%   \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PuzQXfuT8YKV"
   },
   "outputs": [],
   "source": [
    "# Do any columns need standardising?\n",
    "\n",
    "print(\"No standardisation needed as all columns are in standard units.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check the outliers after handling\n",
    "cols_to_check = ['trip_distance', 'fare_amount', 'tip_amount', 'total_amount', 'extra', 'tolls_amount']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(cols_to_check, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(y=df[col])\n",
    "    plt.title(col)\n",
    "    median_val = df[col].median()\n",
    "    plt.axhline(median_val, color='r', linestyle='--', label='Median')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPpJyFFNffcL"
   },
   "source": [
    "## **3** Exploratory Data Analysis\n",
    "<font color = red>[90 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl-0PcYTfkqh"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4N3PvkSTwcN"
   },
   "source": [
    "#### **3.1** General EDA: Finding Patterns and Trends\n",
    "<font color = red>[40 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hwonDfZTJO6"
   },
   "source": [
    "**3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Categorise the varaibles into Numerical or Categorical.\n",
    "* `VendorID`:\n",
    "* `tpep_pickup_datetime`:\n",
    "* `tpep_dropoff_datetime`:\n",
    "* `passenger_count`:\n",
    "* `trip_distance`:\n",
    "* `RatecodeID`:\n",
    "* `PULocationID`:\n",
    "* `DOLocationID`:\n",
    "* `payment_type`:\n",
    "* `pickup_hour`:\n",
    "* `trip_duration`:\n",
    "\n",
    "\n",
    "The following monetary parameters belong in the same category, is it categorical or numerical?\n",
    "\n",
    "\n",
    "* `fare_amount`\n",
    "* `extra`\n",
    "* `mta_tax`\n",
    "* `tip_amount`\n",
    "* `tolls_amount`\n",
    "* `improvement_surcharge`\n",
    "* `total_amount`\n",
    "* `congestion_surcharge`\n",
    "* `airport_fee`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbGMjArkiXjL"
   },
   "source": [
    "##### Temporal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCCbmQ49s2qj"
   },
   "source": [
    "**3.1.2** <font color = red>[5 marks]</font> <br>\n",
    "Analyse the distribution of taxi pickups by hours, days of the week, and months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwgWN_MWg0Au"
   },
   "outputs": [],
   "source": [
    "# Find and show the hourly trends in taxi pickups\n",
    "df['hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "hourly_pickups = df.groupby('hour').size()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=hourly_pickups.index, y=hourly_pickups.values)\n",
    "plt.title('Hourly Taxi Pickups in NYC (Sampled Data)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R64b8zfkG5OK"
   },
   "outputs": [],
   "source": [
    "# Find and show the daily trends in taxi pickups (days of the week)\n",
    "df['day_of_week'] = df['tpep_pickup_datetime'].dt.day_name()\n",
    "daily_pickups = df.groupby('day_of_week').size()\n",
    "# Order the days of the week\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_pickups = daily_pickups.reindex(days_order)   \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=daily_pickups.index, y=daily_pickups.values)\n",
    "plt.title('Daily Taxi Pickups in NYC (Sampled Data)')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7V-jmRQG5hJ"
   },
   "outputs": [],
   "source": [
    "# Show the monthly trends in pickups\n",
    "df['month'] = df['tpep_pickup_datetime'].dt.month\n",
    "monthly_pickups = df.groupby('month').size()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=monthly_pickups.index, y=monthly_pickups.values)\n",
    "plt.title('Monthly Taxi Pickups in NYC (Sampled Data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23IqsgNjHNpJ"
   },
   "source": [
    "##### Financial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRaO-3NqHSM6"
   },
   "source": [
    "Take a look at the financial parameters like `fare_amount`, `tip_amount`, `total_amount`, and also `trip_distance`. Do these contain zero/negative values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7JUnPeRHRqF"
   },
   "outputs": [],
   "source": [
    "# Analyse the above parameters\n",
    "financial_columns = ['fare_amount', 'tip_amount', 'total_amount', 'extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge']\n",
    "\n",
    "# Remove rows where either fare_amount or total_amount is zero\n",
    "original_count = len(df)\n",
    "df = df[(df['fare_amount'] > 0) & (df['total_amount'] > 0)]\n",
    "print(f\"Removed {original_count - len(df)} rows with zero fare or total amount\")\n",
    "print(f\"Remaining rows: {len(df)}\")\n",
    "\n",
    "for col in financial_columns:\n",
    "    if (df[col].lt(0)).any():\n",
    "        print(f\"{col}: values >= zero. Count of negative values: {len(df[df[col] < 0])}, total values : {len(df[col])}\")\n",
    "    else:\n",
    "        print(f\"{col}: values are >= zero.\") \n",
    "\n",
    "# convert the negative values to positive values\n",
    "for col in financial_columns:\n",
    "    if (df[col].lt(0)).any():\n",
    "        df[col] = df[col].abs()\n",
    "        print(f\"Converted negative values in column '{col}' to absolute values.\")   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbOS_-GDJTyv"
   },
   "source": [
    "Do you think it is beneficial to create a copy DataFrame leaving out the zero values from these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### yes ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDN14J63o9gV"
   },
   "source": [
    "**3.1.3** <font color = red>[2 marks]</font> <br>\n",
    "Filter out the zero values from the above columns.\n",
    "\n",
    "**Note:** The distance might be 0 in cases where pickup and drop is in the same zone. Do you think it is suitable to drop such cases of zero distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mk7Yp41JqJd"
   },
   "outputs": [],
   "source": [
    "# Create a df with non zero entries for the selected parameters.\n",
    "cols = ['fare_amount', 'total_amount', 'trip_distance']\n",
    "for col in cols:\n",
    "    original_count = len(df)\n",
    "    df = df[(df[col] > 0)]\n",
    "    new_count = len(df)\n",
    "    print(f\"Percentage dropped: {((original_count - new_count) / original_count) * 100:.2f}% rows with zero {col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJcgwyrtKivH"
   },
   "source": [
    "**3.1.4** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the monthly revenue (`total_amount`) trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_gl8rBD6ZH0"
   },
   "outputs": [],
   "source": [
    "# Group data by month and analyse monthly revenue\n",
    "# Group data by month and analyse monthly revenue\n",
    "monthly_revenue = df.groupby('month')['total_amount'].sum()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=monthly_revenue.index, y=monthly_revenue.values / 1000000)\n",
    "plt.title('Monthly Revenue from NYC Taxi Rides (Sampled Data)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Revenue ($ Millions)')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vQHosTZLalB"
   },
   "source": [
    "**3.1.5** <font color = red>[3 marks]</font> <br>\n",
    "Show the proportion of each quarter of the year in the revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foV9BpFbVTbu"
   },
   "outputs": [],
   "source": [
    "# Calculate proportion of each quarter\n",
    "quarter = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "quarterly_revenue = pd.Series(dtype='float64', index=quarter)\n",
    "quarterly_revenue['Q1'] = monthly_revenue.loc[1:3].sum()\n",
    "quarterly_revenue['Q2'] = monthly_revenue.loc[4:6].sum()\n",
    "quarterly_revenue['Q3'] = monthly_revenue.loc[7:9].sum()\n",
    "quarterly_revenue['Q4'] = monthly_revenue.loc[10:12].sum()\n",
    "\n",
    "total_revenue = quarterly_revenue.sum()\n",
    "quarterly_proportion = (quarterly_revenue / total_revenue) * 100\n",
    "print(\"Quarterly Revenue Proportion (%):\", quarterly_proportion.to_dict())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=quarterly_proportion.index, y=quarterly_proportion.values)\n",
    "plt.title('Quarterly Revenue Proportion from NYC Taxi Rides (Sampled Data)')\n",
    "plt.xlabel('Quarter')\n",
    "plt.ylabel('Revenue Proportion (%)')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JncXEPjBLyHp"
   },
   "source": [
    "**3.1.6** <font color = red>[3 marks]</font> <br>\n",
    "Visualise the relationship between `trip_distance` and `fare_amount`. Also find the correlation value for these two.\n",
    "\n",
    "**Hint:** You can leave out the trips with trip_distance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-z8Fegh_P5At"
   },
   "outputs": [],
   "source": [
    "df_filtered = df[df['fare_amount'] > 0]\n",
    "\n",
    "# Show how trip fare is affected by distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='trip_distance', y='fare_amount', data=df_filtered, alpha=0.3)\n",
    "plt.title('Trip Fare vs. Trip Distance (Sampled Data)')\n",
    "plt.xlabel('Trip Distance (miles)') \n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "correlation = df_filtered['trip_distance'].corr(df_filtered['fare_amount'])\n",
    "print(f\"Correlation between trip distance and fare amount: {correlation:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OGNFTX4QdeS"
   },
   "source": [
    "**3.1.7** <font color = red>[5 marks]</font> <br>\n",
    "Find and visualise the correlation between:\n",
    "1. `fare_amount` and trip duration (pickup time to dropoff time)\n",
    "2. `fare_amount` and `passenger_count`\n",
    "3. `tip_amount` and `trip_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()\n",
    "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60  # in minutes\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtSNqFw19TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between fare and trip duration\n",
    "df.columns.tolist()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='fare_amount', y='trip_duration', data=df, alpha=0.3)\n",
    "plt.title('Trip Fare vs. Trip Duration (Sampled Data)')\n",
    "plt.xlabel('Fare amount($)') \n",
    "plt.ylabel('Trip Duration (minutes)')\n",
    "plt.xlim(0, 100)  # Focus on fares up to $100\n",
    "plt.ylim(0, 100)   # Focus on trips up to 100 minutes\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTrlXiBm9TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between fare and number of passengers\n",
    "df.columns.tolist()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='passenger_count', y='total_amount', data=df, alpha=0.3)\n",
    "plt.title('Total Amount vs. Passenger Count (Sampled Data)')\n",
    "plt.xlabel('Passenger Count') \n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.ylim(0, 100)  # Focus on total amounts up to $100\n",
    "\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lph7rakc9TB3"
   },
   "outputs": [],
   "source": [
    "# Show relationship between tip and trip distance\n",
    "\n",
    "# Show relationship between fare and trip duration\n",
    "df.columns.tolist()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='tip_amount', y='trip_distance', data=df, alpha=0.3)\n",
    "plt.title('Tip vs. Trip Distance (Sampled Data)')\n",
    "plt.xlabel('Tip ($)') \n",
    "plt.ylabel('Distance (miles)')\n",
    "plt.xlim(0, 20)  # Focus on tips up to $20\n",
    "plt.ylim(0, 50)   # Focus on trips up to 50 miles\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EL5CU47QJ5u"
   },
   "source": [
    "**3.1.8** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the distribution of different payment types (`payment_type`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pknOiYLp9Wnd"
   },
   "outputs": [],
   "source": [
    "# Analyse the distribution of different payment types (payment_type).\n",
    "payment_counts = df['payment_type'].value_counts().sort_index()\n",
    "payment_labels = [Payment_type.get(i) for i in payment_counts.index]    \n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=payment_labels, y=payment_counts.values/100000)\n",
    "plt.title('Distribution of Payment Types (Sampled Data)')\n",
    "plt.xlabel('Payment Type')\n",
    "plt.ylabel('Number of Rides (hundred thousand)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxxbUmSZ9Wne"
   },
   "source": [
    "- 1= Credit card\n",
    "- 2= Cash\n",
    "- 3= No charge\n",
    "- 4= Dispute\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVXxcSYHj9sh"
   },
   "source": [
    "##### Geographical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvBXuYW7kEyS"
   },
   "source": [
    "For this, you have to use the *taxi_zones.shp* file from the *taxi_zones* folder.\n",
    "\n",
    "There would be multiple files inside the folder (such as *.shx, .sbx, .sbn* etc). You do not need to import/read any of the files other than the shapefile, *taxi_zones.shp*.\n",
    "\n",
    "Do not change any folder structure - all the files need to be present inside the folder for it to work.\n",
    "\n",
    "The folder structure should look like this:\n",
    "```\n",
    "Taxi Zones\n",
    "|- taxi_zones.shp.xml\n",
    "|- taxi_zones.prj\n",
    "|- taxi_zones.sbn\n",
    "|- taxi_zones.shp\n",
    "|- taxi_zones.dbf\n",
    "|- taxi_zones.shx\n",
    "|- taxi_zones.sbx\n",
    "\n",
    " ```\n",
    "\n",
    " You only need to read the `taxi_zones.shp` file. The *shp* file will utilise the other files by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR8f8ypXUtxJ"
   },
   "source": [
    "We will use the *GeoPandas* library for geopgraphical analysis\n",
    "```\n",
    "import geopandas as gpd\n",
    "```\n",
    "\n",
    "More about geopandas and shapefiles: [About](https://geopandas.org/en/stable/about.html)\n",
    "\n",
    "\n",
    "Reading the shapefile is very similar to *Pandas*. Use `gpd.read_file()` function to load the data (*taxi_zones.shp*) as a GeoDataFrame. Documentation: [Reading and Writing Files](https://geopandas.org/en/stable/docs/user_guide/io.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJ76QD9IXNz8"
   },
   "outputs": [],
   "source": [
    "! pip install geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_QpZLl_WU-b"
   },
   "source": [
    "**3.1.9** <font color = red>[2 marks]</font> <br>\n",
    "Load the shapefile and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLyVd3TQkCdG"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "# Read the shapefile using geopandas\n",
    "zones = gpd.read_file('/home/sirkumar/IIITB/EDA/data_NYC_Taxi/taxi_zones/taxi_zones.shp')\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YXJMzRoWxeg"
   },
   "source": [
    "Now, if you look at the DataFrame created, you will see columns like: `OBJECTID`,`Shape_Leng`, `Shape_Area`, `zone`, `LocationID`, `borough`, `geometry`.\n",
    "<br><br>\n",
    "\n",
    "Now, the `locationID` here is also what we are using to mark pickup and drop zones in the trip records.\n",
    "\n",
    "The geometric parameters like shape length, shape area and geometry are used to plot the zones on a map.\n",
    "\n",
    "This can be easily done using the `plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTqdZChLYE6H"
   },
   "outputs": [],
   "source": [
    "print(zones.info())\n",
    "# Plot the zones map\n",
    "zones.plot(figsize=(12, 12), alpha=0.5, edgecolor='k')\n",
    "plt.title('NYC Taxi Zones')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBkWokLIY_BH"
   },
   "source": [
    "Now, you have to merge the trip records and zones data using the location IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzUMLUjqaJLN"
   },
   "source": [
    "**3.1.10** <font color = red>[3 marks]</font> <br>\n",
    "Merge the zones data into trip data using the `locationID` and `PULocationID` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2L8hWMQaYkX"
   },
   "outputs": [],
   "source": [
    "# Merge zones and trip records using locationID and PULocationID\n",
    "\n",
    "merged_df = df.merge(zones[['LocationID', 'zone']], left_on='PULocationID', right_on='LocationID', how='left')\n",
    "print(\"After merging, merged_df has shape:\", merged_df.shape)\n",
    "print(merged_df.columns.tolist())\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CYc36Weai5-"
   },
   "source": [
    "**3.1.11** <font color = red>[3 marks]</font> <br>\n",
    "Group data by location IDs to find the total number of trips per location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpTIaPSSbwZZ"
   },
   "outputs": [],
   "source": [
    "# Group data by location and calculate the number of trips\n",
    "\n",
    "trip_counts = df.groupby('PULocationID').size().reset_index(name='number of trips')\n",
    "trip_counts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As0dVJpdcK2_"
   },
   "source": [
    "**3.1.12** <font color = red>[2 marks]</font> <br>\n",
    "Now, use the grouped data to add number of trips to the GeoDataFrame.\n",
    "\n",
    "We will use this to plot a map of zones showing total trips per zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9Sheg7vdZ6q"
   },
   "outputs": [],
   "source": [
    "# Merge trip counts back to the zones GeoDataFrame\n",
    "zones_with_trips = pd.merge(zones, trip_counts, left_on='LocationID', right_on='PULocationID', how='left')\n",
    "zones_with_trips.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsfomL6Od0_R"
   },
   "source": [
    "The next step is creating a color map (choropleth map) showing zones by the number of trips taken.\n",
    "\n",
    "Again, you can use the `zones.plot()` method for this. [Plot Method GPD](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.plot.html#geopandas.GeoDataFrame.plot)\n",
    "\n",
    "But first, you need to define the figure and axis for the plot.\n",
    "\n",
    "`fig, ax = plt.subplots(1, 1, figsize = (12, 10))`\n",
    "\n",
    "This function creates a figure (fig) and a single subplot (ax)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgDcw8gUejVk"
   },
   "source": [
    "After setting up the figure and axis, we can proceed to plot the GeoDataFrame on this axis. This is done in the next step where we use the plot method of the GeoDataFrame.\n",
    "\n",
    "You can define the following parameters in the `zones.plot()` method:\n",
    "```\n",
    "column = '',\n",
    "ax = ax,\n",
    "legend = True,\n",
    "legend_kwds = {'label': \"label\", 'orientation': \"<horizontal/vertical>\"}\n",
    "```\n",
    "\n",
    "To display the plot, use `plt.show()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFQmkJipfU1P"
   },
   "source": [
    "**3.1.13** <font color = red>[3 marks]</font> <br>\n",
    "Plot a color-coded map showing zone-wise trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i506kVTgefM5"
   },
   "outputs": [],
   "source": [
    "# Define figure and axis\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 10))\n",
    "legend_kwds = {'label': \"label\", 'orientation': \"<horizontal/vertical>\"}\n",
    "\n",
    "# Plot the map and display zone wise trips\n",
    "zones_with_trips.plot(\n",
    "    column='number of trips', \n",
    "    ax=ax, \n",
    "    legend=True,\n",
    "    legend_kwds={'label': \"Number of Trips by Zone\",'orientation': \"horizontal\"},\n",
    ")\n",
    "plt.title('NYC Taxi Zones - Trip Distribution', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwDs7OaBSopP"
   },
   "outputs": [],
   "source": [
    "# can you try displaying the zones DF sorted by the number of trips?\n",
    "zones_with_trips_sorted = zones_with_trips.sort_values(by='number of trips', ascending=False)\n",
    "print(zones_with_trips_sorted[['zone', 'number of trips']].head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1bE7-QbgSrZ"
   },
   "source": [
    "Here we have completed the temporal, financial and geographical analysis on the trip records.\n",
    "\n",
    "**Compile your findings from general analysis below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YiVFIX3gcL3"
   },
   "source": [
    "You can consider the following points:\n",
    "\n",
    "* Busiest hours, days and months\n",
    "* Trends in revenue collected\n",
    "* Trends in quarterly revenue\n",
    "* How fare depends on trip distance, trip duration and passenger counts\n",
    "* How tip amount depends on trip distance\n",
    "* Busiest zones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv0oYLcbhOTU"
   },
   "source": [
    "#### **3.2** Detailed EDA: Insights and Strategies\n",
    "<font color = red>[50 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWJk-CGihnU1"
   },
   "source": [
    "Having performed basic analyses for finding trends and patterns, we will now move on to some detailed analysis focussed on operational efficiency, pricing strategies, and customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBY2Y4Jrz9IQ"
   },
   "source": [
    "##### Operational Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXU8Q3sGjGnE"
   },
   "source": [
    "Analyze variations by time of day and location to identify bottlenecks or inefficiencies in routes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H97VPqVdq7Lz"
   },
   "source": [
    "**3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Identify slow routes by calculating the average time taken by cabs to get from one zone to another at different hours of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzTRZyw2q9IR"
   },
   "source": [
    "Speed on a route *X* for hour *Y* = (*distance of the route X / average trip duration for hour Y*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovf-1vIyhk7E"
   },
   "outputs": [],
   "source": [
    "# Find routes which have the slowest speeds at different times of the day\n",
    "print(df.columns.to_list())\n",
    "# Check for negative values\n",
    "\n",
    "\n",
    "df_route = df[['PULocationID', 'DOLocationID', 'trip_distance', 'trip_duration', 'tpep_pickup_datetime']].copy() #Make an independent copy otherwise was getting an error\n",
    "df_route['speed_mph'] = (df_route['trip_distance'] / (df_route['trip_duration'] / 60)) #add a column for speed in mph\n",
    "df_route['hour'] = df_route['tpep_pickup_datetime'].dt.hour # add another column for hour\n",
    "print(df_route.tail())\n",
    "# Group by route and hour to calculate average speed\n",
    "# All trips from Zone 161 → Zone 237 at 9 AM\n",
    "# All trips from Zone 161 → Zone 237 at 10 AM\n",
    "# All trips from Zone 48 → Zone 142 at 9 AM\n",
    "# from each group get the speed_mph and take a mean of that\n",
    "average_speeds = df_route.groupby(['PULocationID', 'DOLocationID', 'hour'])['speed_mph'].mean()\n",
    "average_speeds = average_speeds.reset_index()\n",
    "print(average_speeds.sort_values(by='speed_mph').head(20))  # Show the slowest routes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmRPbH3rm2Ub"
   },
   "source": [
    "How does identifying high-traffic, high-demand routes help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-w-OCB_nLmb"
   },
   "source": [
    "**3.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Calculate the number of trips at each hour of the day and visualise them. Find the busiest hour and show the number of trips for that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEbRCT31nLUw"
   },
   "outputs": [],
   "source": [
    "# Visualise the number of trips per hour and find the busiest hour\n",
    "hourly_trip_counts = df.groupby('hour').size()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=hourly_trip_counts.index, y=hourly_trip_counts.values)\n",
    "plt.title('Number of Trips per Hour (Sampled Data)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FuSAA0zn3F4"
   },
   "source": [
    "Remember, we took a fraction of trips. To find the actual number, you have to scale the number up by the sampling ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bapn075QrKaz"
   },
   "source": [
    "**3.2.3** <font color = red>[2 mark]</font> <br>\n",
    "Find the actual number of trips in the five busiest hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79Hy-YWOoapQ"
   },
   "outputs": [],
   "source": [
    "# Scale up the number of trips\n",
    "\n",
    "# Fill in the value of your sampling fraction and use that to scale up the numbers\n",
    "sample_fraction = 0.1  # 5% sampling\n",
    "scaling_factor = 5 / sample_fraction\n",
    "scaled_hourly_trip_counts = hourly_trip_counts * scaling_factor\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=scaled_hourly_trip_counts.index, y=scaled_hourly_trip_counts.values)\n",
    "plt.title('Scaled Number of Trips per Hour (Estimated for Full Data)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Estimated Number of Trips')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74fhoLxEoaTP"
   },
   "source": [
    "**3.2.4** <font color = red>[3 marks]</font> <br>\n",
    "Compare hourly traffic pattern on weekdays. Also compare for weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYMmCQHwpFRY"
   },
   "outputs": [],
   "source": [
    "# Compare traffic trends for the week days and weekends\n",
    "df['is_weekend'] = df['day_of_week'].isin(['Saturday', 'Sunday'])\n",
    "hourly_trips_weekday = df[~df['is_weekend']].groupby('hour').size()\n",
    "hourly_trips_weekend = df[df['is_weekend']].groupby('hour').size()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x=hourly_trips_weekday.index, y=hourly_trips_weekday.values, label='Weekdays')\n",
    "sns.lineplot(x=hourly_trips_weekend.index, y=hourly_trips_weekend.values, label='Weekends')\n",
    "plt.title('Hourly Taxi Trips: Weekdays vs Week ')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Trips')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWeuAulNpSOL"
   },
   "source": [
    "What can you infer from the above patterns? How will finding busy and quiet hours for each day help us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S90VG94rGUm"
   },
   "source": [
    "**3.2.5** <font color = red>[3 marks]</font> <br>\n",
    "Identify top 10 zones with high hourly pickups. Do the same for hourly dropoffs. Show pickup and dropoff trends in these zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nJGifJFrslP"
   },
   "outputs": [],
   "source": [
    "# Find top 10 pickup and dropoff zones\n",
    "top_pickup_zones = df['PULocationID'].value_counts().head(10).reset_index()\n",
    "top_pickup_zones.columns = ['LocationID', 'pickup_count']\n",
    "\n",
    "top_dropoff_zones = df['DOLocationID'].value_counts().head(10).reset_index()\n",
    "top_dropoff_zones.columns = ['LocationID', 'dropoff_count']\n",
    "\n",
    "# Merge with zone names\n",
    "top_pickup_zones = top_pickup_zones.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "top_dropoff_zones = top_dropoff_zones.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "\n",
    "print(\"Top 10 Pickup Zones:\\n\", top_pickup_zones[['zone', 'pickup_count']])\n",
    "print(\"\\nTop 10 Dropoff Zones:\\n\", top_dropoff_zones[['zone', 'dropoff_count']])\n",
    "\n",
    "\n",
    "                                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okkQ17VssMqP"
   },
   "source": [
    "**3.2.6** <font color = red>[3 marks]</font> <br>\n",
    "Find the ratio of pickups and dropoffs in each zone. Display the 10 highest (pickup/drop) and 10 lowest (pickup/drop) ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qbuc8y-KgeDR"
   },
   "outputs": [],
   "source": [
    "# Find the top 10 and bottom 10 pickup/dropoff ratios\n",
    "\n",
    "bottom_pickup_zones = df['PULocationID'].value_counts().tail(10).reset_index()\n",
    "bottom_pickup_zones.columns = ['LocationID', 'pickup_count']\n",
    "\n",
    "bottom_dropoff_zones = df['DOLocationID'].value_counts().tail(10).reset_index()\n",
    "bottom_dropoff_zones.columns = ['LocationID', 'dropoff_count']\n",
    "\n",
    "# Merge with zone names\n",
    "bottom_pickup_zones = bottom_pickup_zones.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "bottom_dropoff_zones = bottom_dropoff_zones.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "\n",
    "print(\"Bottom 10 Pickup Zones:\\n\", bottom_pickup_zones[['zone', 'pickup_count']])\n",
    "print(\"\\nBottom 10 Dropoff Zones:\\n\", bottom_dropoff_zones[['zone', 'dropoff_count']])\n",
    "\n",
    "#Get the ratio of pickups and dropoffs for each zone\n",
    "zone_pickup_counts_df = df.groupby('PULocationID').size().reset_index(name='count')\n",
    "zone_dropoff_counts_df = df.groupby('DOLocationID').size().reset_index(name='count')\n",
    "\n",
    "#Prepare the ground of merge. Rename PU and DO LocationID as LocationID so that they can be merged\n",
    "zone_pickup_counts_df = zone_pickup_counts_df.rename(columns={'PULocationID': 'LocationID', 'count': 'pickup_count'})\n",
    "zone_dropoff_counts_df = zone_dropoff_counts_df.rename(columns={'DOLocationID': 'LocationID', 'count': 'dropoff_count'})\n",
    "zone_counts_df = pd.merge(zone_pickup_counts_df, zone_dropoff_counts_df, on='LocationID', how='outer').fillna(0)\n",
    "print(zone_counts_df.head())\n",
    "\n",
    "#add ratio column for pickups to dropoffs\n",
    "zone_counts_df['pickup_dropoff_ratio'] = zone_counts_df['pickup_count'] / zone_counts_df['dropoff_count']\n",
    "print(zone_counts_df.head())\n",
    "\n",
    "#Now add a column so that we can get the zone names\n",
    "zone_counts_df = zone_counts_df.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "zone_counts_df = zone_counts_df.sort_values(by='pickup_dropoff_ratio', ascending=False)\n",
    "print(\"Top 10 Zones by Pickup/Dropoff Ratio:\\n\", zone_counts_df[['zone', 'pickup_dropoff_ratio']].head(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j1ukv-rrxny"
   },
   "source": [
    "**3.2.7** <font color = red>[3 marks]</font> <br>\n",
    "Identify zones with high pickup and dropoff traffic during night hours (11PM to 5AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve65f0GltyE_"
   },
   "outputs": [],
   "source": [
    "# During night hours (11pm to 5am) find the top 10 pickup and dropoff zones\n",
    "# Note that the top zones should be of night hours and not the overall top zones\n",
    "top_pickup_zones_df = df[(df['hour'] >= 23) | (df['hour'] <= 5)]['PULocationID'].value_counts().sort_values(ascending=False).head(10).reset_index(name='pickup_count')\n",
    "top_dropoff_zones_df = df[(df['hour'] >= 23) | (df['hour'] <= 5)]['DOLocationID'].value_counts().sort_values(ascending=False).head(10).reset_index(name='dropoff_count')\n",
    "\n",
    "#Prepare the ground of merge. Rename PU and DO LocationID as LocationID so that they can be merged\n",
    "top_pickup_zones_df = top_pickup_zones_df.rename(columns={'PULocationID': 'LocationID'})\n",
    "top_dropoff_zones_df = top_dropoff_zones_df.rename(columns={'DOLocationID': 'LocationID'})\n",
    "zone_counts_df = pd.merge(top_pickup_zones_df, top_dropoff_zones_df, on='LocationID', how='outer')\n",
    "#Now add a column so that we can get the zone names\n",
    "top_zone_counts_df = zone_counts_df.merge(zones[['LocationID', 'zone']], on='LocationID', how='left')\n",
    "\n",
    "print(\"Top 10 Pickup Zones during Night Hours:\\n\", top_zone_counts_df[['zone', 'pickup_count']].sort_values(by='pickup_count', ascending=False))\n",
    "print(\"\\nTop 10 Dropoff Zones during Night Hours:\\n\", top_zone_counts_df[['zone', 'dropoff_count']].sort_values(by='dropoff_count', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPTHyhTNnNL"
   },
   "source": [
    "Now, let us find the revenue share for the night time hours and the day time hours. After this, we will move to deciding a pricing strategy.\n",
    "\n",
    "**3.2.8** <font color = red>[2 marks]</font> <br>\n",
    "Find the revenue share for nighttime and daytime hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ohk4curNl3g"
   },
   "outputs": [],
   "source": [
    "# Filter for night hours (11 PM to 5 AM). Find the revenue\n",
    "night_hours_df = df[(df['hour'] >= 23) | (df['hour'] <= 5)]\n",
    "night_revenue = night_hours_df['total_amount'].sum()\n",
    "print(f\"Total Revenue during Night Hours (11 PM to 5 AM): ${night_revenue:,.2f}\")\n",
    "\n",
    "day_hours_df = df[(df['hour'] > 5) & (df['hour'] < 23)]\n",
    "day_revenue = day_hours_df['total_amount'].sum()\n",
    "print(f\"Total Revenue during Day Hours (6 AM to 10 PM): ${day_revenue:,.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG77smP4tyWu"
   },
   "source": [
    "##### Pricing Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciF6eThit5IH"
   },
   "source": [
    "**3.2.9** <font color = red>[2 marks]</font> <br>\n",
    "For the different passenger counts, find the average fare per mile per passenger.\n",
    "\n",
    "For instance, suppose the average fare per mile for trips with 3 passengers is 3 USD/mile, then the fare per mile per passenger will be 1 USD/mile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AO7MIuXd9C16"
   },
   "outputs": [],
   "source": [
    "# Analyse the fare per mile per passenger for different passenger counts\n",
    "df['fare_per_mile_per_passenger'] = df['fare_amount'] / (df['trip_distance'] * df['passenger_count'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='passenger_count', y='fare_per_mile_per_passenger', data=df)\n",
    "plt.title('Fare per Mile per Passenger by Passenger Count (Sampled Data)')\n",
    "plt.xlabel('Passenger Count')\n",
    "plt.ylabel('Fare per Mile per Passenger ($)')\n",
    "plt.ylim(0, 1000)  # Focus on reasonable fare values\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZrN-cxR9DIb"
   },
   "source": [
    "**3.2.10** <font color = red>[3 marks]</font> <br>\n",
    "Find the average fare per mile by hours of the day and by days of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUzCJm2y9SOq"
   },
   "outputs": [],
   "source": [
    "# Compare the average fare per mile for different days and for different times of the day\n",
    "df['fare_per_mile'] = df['fare_amount'] / df['trip_distance']\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='day_of_week', y='fare_per_mile', data=df, order=days_order)\n",
    "plt.title('Fare per Mile by Day of the Week (Sampled Data)')\n",
    "plt.xlabel('Day of the Week')   \n",
    "plt.ylabel('Fare per Mile ($)')\n",
    "plt.ylim(0, 1000)  # Focus on reasonable fare values    \n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='hour', y='fare_per_mile', data=df)\n",
    "plt.title('Fare per Mile by Hour of the Day (Sampled Data)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Fare per Mile ($)')\n",
    "plt.ylim(0, 1000)  # Focus on reasonable fare values\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "211WFkhHDUMG"
   },
   "source": [
    "**3.2.11** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the average fare per mile for the different vendors for different hours of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my3cGHL1DU04"
   },
   "outputs": [],
   "source": [
    "# Compare fare per mile for different vendors\n",
    "df_vendor = df[df['trip_distance'] > 0].copy()\n",
    "df_vendor['fare_per_mile'] = df_vendor['fare_amount'] / df_vendor['trip_distance']\n",
    "vendor_fare_comparison_df = df_vendor.groupby('VendorID')['fare_per_mile'].mean().reset_index()\n",
    "print(vendor_fare_comparison_df.head())\n",
    "\n",
    "#Draw it\n",
    "sns.barplot(x='VendorID', y='fare_per_mile', data=vendor_fare_comparison_df)\n",
    "plt.title('Average Fare per Mile by Vendor (Sampled Data)')\n",
    "plt.xlabel('Vendor ID')\n",
    "plt.ylabel('Average Fare per Mile ($)')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH9YJdodtDk4"
   },
   "source": [
    "**3.2.12** <font color = red>[5 marks]</font> <br>\n",
    "Compare the fare rates of the different vendors in a tiered fashion. Analyse the average fare per mile for distances upto 2 miles. Analyse the fare per mile for distances from 2 to 5 miles. And then for distances more than 5 miles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFRPgO7mgEeT"
   },
   "outputs": [],
   "source": [
    "# Defining distance tiers\n",
    "def distance_tier(distance):\n",
    "    if distance < 2:\n",
    "        return 'Short (<2 miles)'\n",
    "    elif 2 <= distance < 5:\n",
    "        return 'Medium (2-5 miles)'\n",
    "    elif 5 <= distance < 10:\n",
    "        return 'Long (5-10 miles)'\n",
    "    else:\n",
    "        return 'Very Long (10+ miles)'\n",
    "\n",
    "#add a new column for distance tier\n",
    "df['distance_tier'] = df['trip_distance'].apply(distance_tier)\n",
    "\n",
    "\n",
    "#compare the fare rates of the different vendors\n",
    "vendor_fare_comparison_df = df.groupby(['VendorID', 'distance_tier'])['fare_per_mile'].mean().reset_index()\n",
    "print(vendor_fare_comparison_df)\n",
    "# Draw it\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='distance_tier', y='fare_per_mile', hue='VendorID', data=vendor_fare_comparison_df)\n",
    "plt.title('Average Fare per Mile by Vendor and Distance Tier (Sampled Data)')\n",
    "plt.xlabel('Distance Tier')\n",
    "plt.ylabel('Average Fare per Mile ($)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2o6xY0Yh6Gv"
   },
   "source": [
    "##### Customer Experience and Other Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSneN-rCh6Gw"
   },
   "source": [
    "**3.2.13** <font color = red>[5 marks]</font> <br>\n",
    "Analyse average tip percentages based on trip distances, passenger counts and time of pickup. What factors lead to low tip percentages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO5WazK8h6Gx"
   },
   "outputs": [],
   "source": [
    "#  Analyze tip percentages based on distances, passenger counts and pickup times\n",
    "\n",
    "#add tip_percentage column\n",
    "df['tip_percentage'] = (df['tip_amount'] / df['fare_amount']) * 100\n",
    "\n",
    "#average tip percentage by distance tier\n",
    "tip_by_distance_df = df.groupby('distance_tier')['tip_percentage'].mean().reset_index().sort_values(by='tip_percentage', ascending=True)\n",
    "print(tip_by_distance_df)\n",
    "\n",
    "#average tip percentage by passenger count\n",
    "tip_by_passenger_df = df.groupby('passenger_count')['tip_percentage'].mean().reset_index().sort_values(by='tip_percentage', ascending=True)\n",
    "print(tip_by_passenger_df)\n",
    "\n",
    "#average tip percentage by hour\n",
    "tip_by_hour_df = df.groupby('hour')['tip_percentage'].mean().reset_index().sort_values(by='tip_percentage', ascending=True)\n",
    "print(tip_by_hour_df)\n",
    "\n",
    "print(\"Long distance lead to low tip percentages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQAIQsrcLm6X"
   },
   "source": [
    "Additional analysis [optional]: Let's try comparing cases of low tips with cases of high tips to find out if we find a clear aspect that drives up the tipping behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "QYoUed6Dn6YW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Compare trips with tip percentage < 10% to trips with tip percentage > 25%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UjvGTRKh6Gx"
   },
   "source": [
    "**3.2.14** <font color = red>[3 marks]</font> <br>\n",
    "Analyse the variation of passenger count across hours and days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFP0DNRvh6Gx"
   },
   "outputs": [],
   "source": [
    "# See how passenger count varies across hours and days\n",
    "passenger_counts_by_hour = df.groupby('hour')['passenger_count'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='hour', y='passenger_count', data=passenger_counts_by_hour)\n",
    "plt.title('Average Passenger Count by Hour of the Day (Sampled Data)')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Average Passenger Count')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "             \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWKLLlb7h6Gy"
   },
   "source": [
    "**3.2.15** <font color = red>[2 marks]</font> <br>\n",
    "Analyse the variation of passenger counts across zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30h27Uz2h6Gy"
   },
   "outputs": [],
   "source": [
    "# How does passenger count vary across zones\n",
    "passenger_counts_by_zone = df.groupby('PULocationID')['passenger_count'].mean().reset_index()\n",
    "passenger_counts_by_zone = passenger_counts_by_zone.merge(zones[['LocationID', 'zone']], left_on='PULocationID', right_on='LocationID', how='left')\n",
    "passenger_counts_by_zone = passenger_counts_by_zone.sort_values(by='passenger_count', ascending=False)\n",
    "print(\"Top 10 Zones by Average Passenger Count:\\n\", passenger_counts_by_zone[['zone', 'passenger_count']].head(10))\n",
    "\n",
    "\n",
    "# Plot the map and display zone wise average passenger counts\n",
    "sns.barplot(x='passenger_count', y='zone', data=passenger_counts_by_zone.head(20))\n",
    "plt.title('NYC Taxi Zones - Average Passenger Count', fontsize=16)\n",
    "plt.xlabel('Average Passenger Count')\n",
    "plt.ylabel('Zone')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3I0AfOkvBWA"
   },
   "outputs": [],
   "source": [
    "# For a more detailed analysis, we can use the zones_with_trips GeoDataFrame\n",
    "# Create a new column for the average passenger count in each zone.\n",
    "\n",
    "passenger_avg_by_zone = df.groupby('PULocationID')['passenger_count'].mean().reset_index()\n",
    "passenger_avg_by_zone.columns = ['LocationID', 'avg_passenger_count']\n",
    "\n",
    "zones_with_passenger_counts = zones.merge(passenger_avg_by_zone, on='LocationID', how='left')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "\n",
    "zones_with_passenger_counts.plot(\n",
    "    column='avg_passenger_count',\n",
    "    ax=ax,\n",
    "    legend=True,\n",
    "    cmap='YlOrRd',\n",
    "    legend_kwds={'label': \"Average Passenger Count by Zone\", 'orientation': \"horizontal\"},\n",
    ")\n",
    "plt.title('NYC Taxi Zones - Average Passenger Count', fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0j5Z-1h6Gz"
   },
   "source": [
    "Find out how often surcharges/extra charges are applied to understand their prevalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9IHjHFBx6Ye"
   },
   "source": [
    "**3.2.16** <font color = red>[5 marks]</font> <br>\n",
    "Analyse the pickup/dropoff zones or times when extra charges are applied more frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKbXwZJwh6Gz"
   },
   "outputs": [],
   "source": [
    "# How often is each surcharge applied?\n",
    "surcharge_cols = ['extra', 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge']\n",
    "surcharge_application = {}\n",
    "# zones where surcharge are applied more frequently\n",
    "surcharge_frequency = df[surcharge_cols].astype(bool).sum() / len(df) * 100\n",
    "\n",
    "print(\"Frequency of Surcharge Application (%):\")\n",
    "print(surcharge_frequency)\n",
    "\n",
    "# Count rows where tolls_amount is more than zero\n",
    "tolls_count = (df['tolls_amount'] > 0).sum()\n",
    "total_rows = len(df)\n",
    "percentage = (tolls_count / total_rows) * 100\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(surcharge_frequency.index, surcharge_frequency.values)\n",
    "plt.title('Frequency of Surcharge Application')\n",
    "plt.xlabel('Surcharge Type')\n",
    "plt.ylabel('Frequency (%)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkyc9q40Cts-"
   },
   "source": [
    "## **4** Conclusion\n",
    "<font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5yCODI-C6yR"
   },
   "source": [
    "### **4.1** Final Insights and Recommendations\n",
    "<font color = red>[15 marks]</font> <br>\n",
    "\n",
    "Conclude your analyses here. Include all the outcomes you found based on the analysis.\n",
    "\n",
    "Based on the insights, frame a concluding story explaining suitable parameters such as location, time of the day, day of the week etc. to be kept in mind while devising a strategy to meet customer demand and optimise supply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4** Conclusion Summary\n",
    "\n",
    "### Key Findings from the Analysis:\n",
    "\n",
    "#### **Temporal Patterns:**\n",
    "- **Peak Hours**: Taxi demand peaks during evening rush hours (6-8 PM) and morning commute (8-9 AM)\n",
    "- **Weekday vs Weekend**: Weekdays show distinct morning and evening peaks, while weekends have more consistent demand throughout the day\n",
    "- **Monthly Trends**: Demand remains relatively stable across months with slight variations in Q1 and Q4\n",
    "- **Night Operations**: Significant revenue (approximately 15-20%) comes from night hours (11 PM - 5 AM)\n",
    "\n",
    "#### **Financial Insights:**\n",
    "- Strong positive correlation (0.85+) between trip distance and fare amount\n",
    "- Quarterly revenue shows consistent patterns with Q2 and Q3 typically generating higher revenues\n",
    "- Average fare per mile varies significantly by time of day and location\n",
    "- Vendor pricing strategies differ, particularly for short vs. long-distance trips\n",
    "\n",
    "#### **Geographical Patterns:**\n",
    "- Manhattan (especially Midtown, Upper East Side) dominates pickup and dropoff activity\n",
    "- Airport zones (JFK, LaGuardia) show high pickup/dropoff ratios\n",
    "- Certain zones act as \"source\" zones (high pickups) while others are \"destination\" zones (high dropoffs)\n",
    "- Night-time demand concentrates in entertainment districts and residential areas\n",
    "\n",
    "#### **Customer Behavior:**\n",
    "- Credit card payments dominate (70%+), with cash as secondary\n",
    "- Average passenger count is 1-2 passengers per trip\n",
    "- Tip percentages decrease with longer distances\n",
    "- Customer satisfaction appears linked to trip efficiency and duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dy8J-C8jJjk"
   },
   "source": [
    "**4.1.1** <font color = red>[5 marks]</font> <br>\n",
    "Recommendations to optimize routing and dispatching based on demand patterns and operational inefficiencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routing and Dispatching Optimization:\n",
    "\n",
    "**1. Time-Based Dispatching Strategy:**\n",
    "- **Morning Rush (7-9 AM)**: Pre-position 40% of fleet in residential zones (Queens, Brooklyn) for Manhattan-bound trips\n",
    "- **Evening Rush (5-8 PM)**: Concentrate 45% of fleet in business districts for outbound trips\n",
    "- **Mid-day (10 AM-2 PM)**: Distribute fleet evenly across high-demand zones with focus on tourist areas\n",
    "\n",
    "**2. Route Efficiency Improvements:**\n",
    "- Identify and avoid slow routes during peak hours (average speed < 10 mph)\n",
    "- Implement dynamic routing based on real-time traffic data\n",
    "- Focus on high-speed corridors during off-peak hours to maximize trip volume\n",
    "- Prioritize routes with consistent demand and minimal congestion\n",
    "\n",
    "**3. Zone-Based Allocation:**\n",
    "- **High-Pickup Zones**: Ensure 60% fleet coverage in Midtown Manhattan, Upper East/West Side\n",
    "- **Balanced Zones**: Maintain 25% coverage in zones with equal pickup/dropoff ratios\n",
    "- **Destination Zones**: Keep 15% reserve fleet for return trips from high-dropoff areas\n",
    "\n",
    "**4. Predictive Dispatching:**\n",
    "- Use historical patterns to anticipate demand surges\n",
    "- Pre-position cabs 15-20 minutes before expected peak periods\n",
    "- Implement surge pricing alerts for drivers to move to high-demand areas\n",
    "\n",
    "**5. Efficiency Metrics:**\n",
    "- Target: Reduce average wait time by 20% through better positioning\n",
    "- Goal: Increase trips per vehicle per day by 15% through optimized routing\n",
    "- Monitor: Dead-head miles (empty cab travel) and reduce by 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J03px17x_rD9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaLgTWxpjt7h"
   },
   "source": [
    "**4.1.2** <font color = red>[5 marks]</font> <br>\n",
    "\n",
    "Suggestions on strategically positioning cabs across different zones to make best use of insights uncovered by analysing trip trends across time, days and months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategic Cab Positioning Strategy:\n",
    "\n",
    "**1. Weekday Positioning (Monday-Friday):**\n",
    "\n",
    "**Morning Strategy (6 AM - 12 PM):**\n",
    "- **7-9 AM**: Position 40% of fleet in residential boroughs (Queens, Brooklyn, Bronx)\n",
    "- **9-11 AM**: Shift 30% to business districts for short business trips\n",
    "- **11 AM-12 PM**: Redistribute to lunch hotspots and tourist areas\n",
    "\n",
    "**Afternoon/Evening Strategy (12 PM - 10 PM):**\n",
    "- **12-2 PM**: Concentrate 25% near restaurants and business districts\n",
    "- **5-8 PM**: Deploy 50% in Manhattan business zones for evening commute\n",
    "- **8-10 PM**: Move 30% to entertainment districts (Theater District, Greenwich Village)\n",
    "\n",
    "**Night Strategy (10 PM - 6 AM):**\n",
    "- **10 PM-2 AM**: Focus 40% on entertainment zones and nightlife areas\n",
    "- **2-6 AM**: Reduce active fleet to 30%, concentrate near airports and hotels\n",
    "\n",
    "**2. Weekend Positioning (Saturday-Sunday):**\n",
    "- **Morning (8 AM-12 PM)**: Distribute 40% in residential areas and brunch spots\n",
    "- **Afternoon (12-6 PM)**: Focus 45% on shopping districts, tourist attractions, parks\n",
    "- **Evening (6 PM-2 AM)**: Concentrate 50% in entertainment and dining areas\n",
    "- **Night (2-8 AM)**: Maintain 20% active fleet near airports and late-night venues\n",
    "\n",
    "**3. Monthly/Seasonal Adjustments:**\n",
    "- **Q1 (Jan-Mar)**: Increase airport coverage by 15% due to holiday travel returns\n",
    "- **Q2 (Apr-Jun)**: Boost tourist area coverage by 20% as weather improves\n",
    "- **Q3 (Jul-Sep)**: Maintain high coverage in all zones due to peak tourism\n",
    "- **Q4 (Oct-Dec)**: Increase coverage in shopping districts by 25% during holidays\n",
    "\n",
    "**4. Special Event Planning:**\n",
    "- Monitor major events (concerts, sports, conventions) and pre-position 100+ cabs\n",
    "- Create dedicated pick-up zones near venues\n",
    "- Extend night operations by 2-3 hours post-event\n",
    "\n",
    "**5. Zone-Specific Strategies:**\n",
    "\n",
    "**High-Pickup Zones (Upper East Side, Midtown, Penn Station):**\n",
    "- Maintain constant minimum coverage of 15-20 cabs\n",
    "- Rapid replenishment after dispatches\n",
    "\n",
    "**Airport Zones (JFK, LaGuardia):**\n",
    "- Dedicated airport fleet of 10% total vehicles\n",
    "- Coordinate with flight schedules for arrivals\n",
    "\n",
    "**Balanced Zones (Chelsea, Financial District):**\n",
    "- Dynamic allocation based on real-time demand\n",
    "- 10-15% coverage during business hours\n",
    "\n",
    "**6. Technology Integration:**\n",
    "- Implement heat maps showing real-time demand\n",
    "- GPS tracking for optimal repositioning\n",
    "- Driver incentives for moving to under-served zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8ZbTIF7_rsN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUU4mn50jMZy"
   },
   "source": [
    "**4.1.3** <font color = red>[5 marks]</font> <br>\n",
    "Propose data-driven adjustments to the pricing strategy to maximize revenue while maintaining competitive rates with other vendors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Pricing Strategy:\n",
    "\n",
    "**1. Time-Based Pricing Adjustments:**\n",
    "\n",
    "**Peak Hour Pricing (Surge Multipliers):**\n",
    "- **Morning Rush (7-9 AM)**: Base fare + 20% surcharge\n",
    "- **Evening Rush (5-8 PM)**: Base fare + 25% surcharge\n",
    "- **Night Hours (12 AM-5 AM)**: Base fare + 30% for safety and scarcity\n",
    "- **Off-Peak (10 AM-4 PM, 9 PM-12 AM)**: Base fare (standard rate)\n",
    "\n",
    "**Day-of-Week Pricing:**\n",
    "- **Monday-Thursday**: Standard rates\n",
    "- **Friday Evening**: +15% from 5 PM onwards\n",
    "- **Saturday**: +10% during peak evening hours (7 PM-2 AM)\n",
    "- **Sunday**: Standard rates with slight reduction (-5%) during afternoon to boost demand\n",
    "\n",
    "**2. Distance-Based Tiered Pricing:**\n",
    "\n",
    "**Short Trips (<2 miles):**\n",
    "- Minimum fare: $8-10 (covers operational costs)\n",
    "- Per-mile rate: $3.50-4.00\n",
    "- Justification: High frequency, quick turnaround\n",
    "\n",
    "**Medium Trips (2-5 miles):**\n",
    "- Base fare: $5-6\n",
    "- Per-mile rate: $3.00-3.25\n",
    "- Sweet spot for revenue optimization\n",
    "\n",
    "**Long Trips (5-10 miles):**\n",
    "- Base fare: $6-7\n",
    "- Per-mile rate: $2.75-3.00\n",
    "- Volume discount to remain competitive\n",
    "\n",
    "**Very Long Trips (10+ miles):**\n",
    "- Base fare: $7-8\n",
    "- Per-mile rate: $2.50-2.75\n",
    "- Competitive with app-based services\n",
    "\n",
    "**3. Zone-Based Pricing:**\n",
    "\n",
    "**High-Demand Zones (Midtown, Financial District):**\n",
    "- Peak times: +20% premium\n",
    "- Justification: High demand, limited supply\n",
    "\n",
    "**Airport Trips:**\n",
    "- Flat-rate options to JFK/LaGuardia\n",
    "- Competitive with ride-sharing: $52-70 depending on zone\n",
    "- Additional $5 airport fee (mandatory)\n",
    "\n",
    "**Bridge/Tunnel Zones:**\n",
    "- Transparently pass toll costs\n",
    "- Small premium (+$2-3) for outer borough trips\n",
    "\n",
    "**4. Passenger-Based Pricing:**\n",
    "- Base rate: 1-2 passengers\n",
    "- 3+ passengers: +$1-2 per additional passenger after 2\n",
    "- Group discount for 4+ passengers: Waive extra passenger fee\n",
    "\n",
    "**5. Competitive Positioning:**\n",
    "\n",
    "**Vendor Benchmarking:**\n",
    "- Monitor Vendor 1 vs Vendor 2 pricing\n",
    "- Match competitive rates on popular routes\n",
    "- Differentiate on service quality, not just price\n",
    "\n",
    "**Ride-Share Competition:**\n",
    "- Implement dynamic pricing algorithm similar to Uber/Lyft\n",
    "- Advantage: Professional drivers, regulated service\n",
    "- Price matching during off-peak to capture market share\n",
    "\n",
    "**6. Revenue Optimization Strategies:**\n",
    "\n",
    "**Minimum Revenue per Trip:**\n",
    "- Target: $15-20 average fare per trip\n",
    "- Current: ~$17 (maintain or improve)\n",
    "\n",
    "**Trip Duration Pricing:**\n",
    "- Add time-based component: $0.50 per minute in traffic\n",
    "- Compensates for time lost in congestion\n",
    "\n",
    "**Congestion Pricing Integration:**\n",
    "- Work with city congestion surcharges\n",
    "- Transparent pass-through to passengers\n",
    "- Additional $2.50-3.50 during congestion zone hours\n",
    "\n",
    "**7. Promotional Pricing:**\n",
    "\n",
    "**Off-Peak Incentives:**\n",
    "- 10% discount for trips booked 15+ minutes in advance during slow periods\n",
    "- Loyalty program: 10th ride free for regular customers\n",
    "\n",
    "**Tourist Packages:**\n",
    "- Day passes for multiple trips\n",
    "- Fixed pricing for popular tourist routes\n",
    "\n",
    "**Corporate Partnerships:**\n",
    "- Negotiated rates for business accounts\n",
    "- Monthly billing, slight discount (5-8%)\n",
    "\n",
    "**8. Technology-Enabled Pricing:**\n",
    "\n",
    "**Real-Time Adjustments:**\n",
    "- Algorithm monitors: demand, weather, events, traffic\n",
    "- Auto-adjust pricing every 5-10 minutes\n",
    "- Cap maximum surge at 2.5x base rate\n",
    "\n",
    "**Transparent Pricing:**\n",
    "- Upfront fare estimates via app\n",
    "- No surprise charges\n",
    "- Build trust = customer retention\n",
    "\n",
    "**9. Financial Targets:**\n",
    "- **Revenue Growth**: Increase revenue per vehicle by 12-15% annually\n",
    "- **Market Share**: Capture 35-40% of taxi market through competitive pricing\n",
    "- **Customer Satisfaction**: Maintain 85%+ satisfaction while optimizing prices\n",
    "- **Driver Earnings**: Ensure drivers earn 15-20% more through higher trip volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXErHFjx_sGN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-OVfUMlHFkZD",
    "0eaCZjHIvfuI",
    "Kw-WRzBfyS7j",
    "nM2X-s6lycvQ",
    "NgHgbPIepaYl",
    "QaOS3H9izZ0N",
    "HZvPSwJx0S3K",
    "x2hakCCy6wXI",
    "2jyQyYIpCztl",
    "gPpJyFFNffcL",
    "a4N3PvkSTwcN",
    "MbGMjArkiXjL",
    "23IqsgNjHNpJ",
    "CVXxcSYHj9sh",
    "Rv0oYLcbhOTU",
    "XBY2Y4Jrz9IQ",
    "nG77smP4tyWu",
    "M2o6xY0Yh6Gv",
    "bkyc9q40Cts-",
    "Z5yCODI-C6yR"
   ],
   "provenance": [
    {
     "file_id": "1PvYLECban8pmD-aFGV_yoRF2PyRwLntw",
     "timestamp": 1737110016234
    },
    {
     "file_id": "1jppXTxMvUcVsY27R_ckdE5DAnxrROUY1",
     "timestamp": 1732620370544
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python3.12_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
