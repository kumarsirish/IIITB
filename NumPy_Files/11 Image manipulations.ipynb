{"cells":[{"cell_type":"markdown","metadata":{"id":"Oc1dxRcyqDLs"},"source":["In computing, images are represented by arrays of pixels, where each pixel contains information about colour and brightness. We can manipulate images at the pixel level using libraries such as NumPy. Let's start by importing the libraries we require."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-M3HJgfp2Rk"},"outputs":[],"source":["from PIL import Image  # importing the Image module from the Python Imaging Library\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"3YV0LUlCrJR_"},"source":["We can open images using the Python Imaging Library (PIL). Red, green, and blue are the primary colours, and a combination of these is enough to generate any colour."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oHF9XDc_rnPr"},"outputs":[],"source":["image = Image.open('tractor_img.jpg')\n","image"]},{"cell_type":"markdown","metadata":{"id":"jWSUtV7msjJC"},"source":["We can now convert the image to an array, which will allow us to work directly with its pixel values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVEFSgN_sihh"},"outputs":[],"source":["image_arr = np.array(image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrqaLfqLuAXI"},"outputs":[],"source":["image_arr"]},{"cell_type":"markdown","metadata":{"id":"vs17X5ptswAg"},"source":["Let's check out the shape of this array"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5FvBxl4s7JB"},"outputs":[],"source":["image_arr.shape  # height x width x channels"]},{"cell_type":"markdown","metadata":{"id":"hGktuWgStKnG"},"source":["The array is a 3D array with the dimensions (height, width, 3), where each pixel is represented by a set of three values corresponding to the RGB channels. Let's perform some simple image manipulations on our image."]},{"cell_type":"markdown","metadata":{"id":"8KVx7j3ot0tf"},"source":["### Inversion\n","\n","RGB values take the range $[0, 255]$ for each colour. (0, 0, 0) for (R, G, B) corresponds to black, or the complete absence of colour, while (255, 255, 255) corresponds to white. To invert an image, therefore, we need to subtract each pixel value from 255. Thus, the inversion of white would be black while the inversion of black would be white."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMet_Okjs7o_"},"outputs":[],"source":["inverted_image_arr = 255 - image_arr"]},{"cell_type":"markdown","metadata":{"id":"pMOzbkhgvub9"},"source":["We can now convert the array back to an image and view it to verify our result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpDL0fA2tbTc"},"outputs":[],"source":["inverted_image = Image.fromarray(inverted_image_arr)\n","inverted_image"]},{"cell_type":"markdown","metadata":{"id":"Ptuh1bgpwryB"},"source":["### Convert to greyscale\n","\n","To convert an image to greyscale, we need to apply a formula which will weigh each channel to arrive at the range of colours that the human eye approximately interprets as grey. The formula is as follows:\n","\n","$$grey = 0.2989 \\cdot R + 0.5870 \\cdot G + 0.1140 \\cdot B$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbTeBOEgw0RT"},"outputs":[],"source":["greyscale_image_arr = 0.2989 * image_arr[:, :, 0] + 0.5870 * image_arr[:, :, 1] + 0.1140 * image_arr[:, :, 2]\n","\n","greyscale_image_arr = greyscale_image_arr.astype(np.uint8)  # as Image.fromarray() cannot handle float values\n","\n","greyscale_image = Image.fromarray(greyscale_image_arr)\n","greyscale_image"]},{"cell_type":"markdown","metadata":{"id":"LtkRxdnoyquH"},"source":["### Adjusting brightness\n","Brightness can be adjusted by simply adding more of the colour value to the image. We need to ensure that the range of values does not leave $[0, 255]$; to do this, we can use the `np.clip()` function, which clips values based on lower and upper bounds."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wM6oFK-zM2y"},"outputs":[],"source":["brightness_factor = 1.5  # increasing brightness by 50%\n","brightened_image_arr = np.clip(image_arr * brightness_factor, 0, 255).astype(np.uint8)\n","\n","brightened_image = Image.fromarray(brightened_image_arr)\n","brightened_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vefDPiMGzbVZ"},"outputs":[],"source":["brightness_factor = 0.5  # decreasing brightness by 50%\n","darkened_image_arr = np.clip(image_arr * brightness_factor, 0, 255).astype(np.uint8)\n","\n","darkened_image = Image.fromarray(darkened_image_arr)\n","darkened_image"]},{"cell_type":"markdown","metadata":{"id":"VEjaRPRxztkt"},"source":["### Adjusting contrast\n","\n","Contrast can be adjusted by scaling pixel values around the mean image brightness. Increasing the distance from the mean for each pixel boosts contrast."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmRebJfz642P"},"outputs":[],"source":["mean = np.mean(image_arr)  # find the mean image brightness\n","contrast_factor = 0.5  # increase contrast by 80%\n","contrasted_image_arr = np.clip((image_arr - mean) * contrast_factor + mean, 0, 255).astype(np.uint8)\n","\n","contrasted_image = Image.fromarray(contrasted_image_arr)\n","contrasted_image"]},{"cell_type":"markdown","metadata":{"id":"Hnkstsxs9S8w"},"source":["### Filtering colours\n","\n","We can turn off some colour channels and leave the rest on. For instance, we could just leave the red channel on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kf_vqx4g9t6A"},"outputs":[],"source":["red_channel_image_arr = image_arr.copy()  # shallow copy of the array to prevent changes from modifying the original array\n","red_channel_image_arr[:, :, 1] = 0  # set green channel to 0\n","red_channel_image_arr[:, :, 2] = 0  # set blue channel to 0\n","\n","red_channel_image = Image.fromarray(red_channel_image_arr)\n","red_channel_image"]},{"cell_type":"markdown","metadata":{"id":"Y9D4zQll90m1"},"source":["### Rotating an image\n","\n","We can rotate an image by using the `np.rot90()` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePjPlWdG-4Wf"},"outputs":[],"source":["acw_rotated_image_arr = np.rot90(image_arr, k = 3)  # k = 1 represents anti-clockwise rotation\n","\n","acw_rotated_image = Image.fromarray(acw_rotated_image_arr)\n","acw_rotated_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTVZIAXK9zGV"},"outputs":[],"source":["cw_rotated_image_arr = np.rot90(image_arr, k = -1)  # k = -1 represents clockwise rotation\n","\n","cw_rotated_image = Image.fromarray(cw_rotated_image_arr)\n","cw_rotated_image"]},{"cell_type":"markdown","metadata":{"id":"okD8rLeB-ZRP"},"source":["### Flipping an image\n","\n","We can flip an image about the horizontal or vertical axis by using the `np.flip()` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TB4UZLvj-YFs"},"outputs":[],"source":["# Horizontal flip\n","h_flipped_image_arr = np.flip(image_arr, axis = 1)\n","\n","h_flipped_image = Image.fromarray(h_flipped_image_arr)\n","h_flipped_image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHzrlPp6-kvv"},"outputs":[],"source":["# Vertical flip\n","v_flipped_image_arr = np.flip(image_arr, axis = 0)\n","\n","v_flipped_image = Image.fromarray(v_flipped_image_arr)\n","v_flipped_image"]},{"cell_type":"markdown","metadata":{"id":"hZTCqlnaGxn6"},"source":["### Kernels or filters\n","Kernel-based or filter-based methods are a set of methods in image processing that use the available pixel value data along with the relative positions to perform various tasks such as blurring, edge detection, and so on.\n","\n","We can think of kernels as boxes of a limited size, say `kernel_size`, moving across the image, and performing these operations.\n","\n","Say we want to achieve blurring. The box must have a well-defined center pixel in this case because we want to substitute new values for the pixels, thus the kernel size should be an odd number.\n","\n","We need to avoid the box leaving the bounds of the image. As the box centers itself on a pixel, we must ensure it starts iterating from `kernel_size // 2` after the first pixel and the same value before the last pixel, on both, the height and width dimensions.\n","\n","Let's look at this using our greyscale version of the image"]},{"cell_type":"markdown","metadata":{"id":"aMJHFtqUdEK8"},"source":["Returning to our image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYQ9AtUrKw4L"},"outputs":[],"source":["kernel_size = 5  # must be odd as it centers on a pixel\n","hk = kernel_size // 2  # 'half kernel'; this helps in determining the neighborhood range\n","\n","# Create an empty array of the same shape as the original to store the blurred image\n","blurred_image_arr = np.zeros(greyscale_image_arr.shape)\n","\n","# Apply a box blur to the central area of the image (i.e., minus the edges)\n","for i in range(hk, greyscale_image_arr.shape[0] - hk):  # iterating over image height\n","    for j in range(hk, greyscale_image_arr.shape[1] - hk):  # iterating over image width\n","\n","        # Extract the neighborhood defined by the kernel size\n","        neighborhood = greyscale_image_arr[\n","            i - hk : i + hk + 1,  # +1 to account for the fact that the end index is exclusive\n","            j - hk : j + hk + 1\n","        ]\n","\n","        # Calculate the mean of the neighborhood for each color channel\n","        blurred_image_arr[i, j] = np.mean(neighborhood)\n","\n","\n","blurred_image_arr = blurred_image_arr.astype(np.uint8)  # as Image.fromarray() cannot handle float values\n","\n","# Convert the output array back to an image and display it\n","blurred_image = Image.fromarray(blurred_image_arr)\n","blurred_image"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}