{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Importing the Pandas library"],"metadata":{"id":"gH9KWzM74eJh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_WEMy8p1xPn"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"markdown","source":["Let's have a look at the Shark Tank India dataset. Shark Tank is a show where enterpreneurs pitch their ideas on live television to a group of investors, who then compete to invest money in these ideas. The first dataset is a dataset of the pitches made, and the deal values that the pitches were closed at. All money values are in lakhs of rupees."],"metadata":{"id":"THiEnVK_4f7q"}},{"cell_type":"code","source":["df_pitches = pd.read_csv('shark_tank_india_pitches.csv')\n","df_pitches.head()"],"metadata":{"id":"k452AHOG4qd_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The second dataset is a breakdown of the deal closures in terms of the sharks (investors) who invested"],"metadata":{"id":"sttzB2M181WY"}},{"cell_type":"code","source":["df_sharks = pd.read_csv('shark_tank_india_sharks.csv')\n","df_sharks.head()"],"metadata":{"id":"luv_V2u-9BDC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the `pitch_number` column is common between both datasets. We can therefore merge the two DataFrames on this column"],"metadata":{"id":"MKDNoZGb98Sg"}},{"cell_type":"code","source":["df = df_pitches.merge(df_sharks, on = 'pitch_number')\n","df.head()"],"metadata":{"id":"fwkXyymF97DS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We could also achieve the same using the `.join()` method. However, this would require us to explicitly set our 'pitch_number' column as an index."],"metadata":{"id":"-hXIUN_YRuU-"}},{"cell_type":"code","source":["# df2 = df_pitches.join(df_sharks)"],"metadata":{"id":"gKAHYgN9SHSD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_pitches_2 = df_pitches.set_index('pitch_number')\n","df_sharks_2 = df_sharks.set_index('pitch_number')\n","df2 = df_pitches_2.join(df_sharks_2)\n","df2.head()"],"metadata":{"id":"I3IRf8hATGbl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Are our two merged DataFrames equal?"],"metadata":{"id":"xkROljv8Tp82"}},{"cell_type":"code","source":["df.equals(df2)"],"metadata":{"id":"5Zu4oKLJTtLx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's reset our index to remove 'pitch_number' as an index"],"metadata":{"id":"vIttJj1TTWNg"}},{"cell_type":"code","source":["df2.reset_index(inplace = True)"],"metadata":{"id":"OJaYzQUNTTq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.equals(df2)"],"metadata":{"id":"wzbKAyhZSKbw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like the order of the columns after resetting our index has changed. Let's swap the order of our first two columns."],"metadata":{"id":"DdKtZz_SToyh"}},{"cell_type":"code","source":["cols = list(df2.columns)\n","cols[0], cols[1] = cols[1], cols[0]\n","df2 = df2[cols]\n","df2.head()"],"metadata":{"id":"cXhsL3t2UCAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.equals(df2)"],"metadata":{"id":"XyWBDEJtUZN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generally, `.merge()` tends to be far more flexible than `.join()`, so we can avoid using the latter. Anyway, let's return to our DataFrame."],"metadata":{"id":"CFtW7xqQ9S0Y"}},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"EqPt6P3x9USx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"dWm2pgeV9VMU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","How could we calculate the average difference between the pitcher ask amount and the deal amount?"],"metadata":{"id":"Xv9oUkmXUgXe"}},{"cell_type":"code","source":["df['ask_deal_diff'] = df['pitcher_ask_amount'] - df['deal_amount']  # making a new column with the difference\n","df['ask_deal_diff'].mean()"],"metadata":{"id":"vg46jQV8-IkP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['ask_deal_diff'].describe()"],"metadata":{"id":"2TBl0faZKz5X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This seems like a very high figure, let's see if there are some potential outliers in our dataset causing the difference between ask and deal amounts to be so high. While we can manually inspect and look for data points in this case, for larger datasets, it would be preferable to employ a statistical method. Let's try to implement one such method.\n","\n","The interquartile range, or IQR, is the range in which the middle 50% of the data falls. To calculate the IQR, you need to find the difference between the first and third quartiles of your data. The first quartile (Q1) is the value below which 25% of our data lies; the third quartile (Q3) is the value below which 75% of our data lies. We can calculate these using the `.quantile()` method.\n","\n","To remove potential outliers using the IQR method, we remove the data that is outside the following boundaries:\n","\n","*   $Lower = Q1 - 1.5 \\times IQR$\n","*   $Upper = Q3 + 1.5 \\times IQR$\n","\n","**Note:** In Pandas, `|` is used instead of `or` to perform the OR operation, signifying element-wise logical operations on multiple elements rather than on single booleans. Furthermore, parantheses are necessary while chaining multiple logical operations in Pandas.\n"],"metadata":{"id":"4AnCclV2A6VU"}},{"cell_type":"code","source":["# Finding the first (Q1) and third (Q3) quartiles\n","Q1 = df['ask_deal_diff'].quantile(0.25)  # 25% of our observations lie below this figure\n","Q3 = df['ask_deal_diff'].quantile(0.75)  # 75% of our observations lie below this figure\n","IQR = Q3 - Q1  # interquartile range (difference between third and first quartiles)\n","\n","# Define the bounds for outliers\n","lower_bound = Q1 - 1.5 * IQR\n","upper_bound = Q3 + 1.5 * IQR\n","\n","# Finding the outliers\n","df_outliers = df[(df['ask_deal_diff'] < lower_bound) | (df['ask_deal_diff'] > upper_bound)]\n","df_outliers"],"metadata":{"id":"BgbGL0heASIO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As none of these deals were closed, we can choose to remove them from our dataset"],"metadata":{"id":"lYgEF0KxCj-S"}},{"cell_type":"code","source":["df_no_outliers = df[(df['ask_deal_diff'] >= lower_bound) & (df['ask_deal_diff'] <= upper_bound)]\n","df_no_outliers['ask_deal_diff'].mean()  # finding the mean difference between ask and deal amounts again"],"metadata":{"id":"3eAdX_fVAcHW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This feels like a reasonable figure. Now, let's try to find the average participation rates for each shark. We will return to using our original DataFrame as the outliers we calculated were to do with the difference between ask and deal amounts; we would still like to include these entries as cases where sharks did not participate in deals. Let's output the participation rates as percentages in dictionaries."],"metadata":{"id":"RkuMRd2dD33J"}},{"cell_type":"code","source":["# Let's identify the sharks who participated in closing a deal\n","sharks = ['ashneer_deal', 'anupam_deal', 'aman_deal', 'namita_deal', 'vineeta_deal', 'peyush_deal', 'ghazal_deal']\n","participation_rates = {shark.split('_')[0].capitalize(): f'{df[shark].mean() * 100:.2f}%' for shark in sharks}\n","participation_rates"],"metadata":{"id":"9-rMvCMLDyL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Thus, we can see that Aman and Peyush participated in the most number of deals, while Ghazal participated in the least. However, this might not be a fair assessment as we haven't taken into account whether a shark was present during the episode of a deal or not. Luckily, we have columns to account for this."],"metadata":{"id":"Wi6YgIRQF22X"}},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"YtrKrakIMMSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["shark_presence = ['ashneer_present', 'anupam_present', 'aman_present', 'namita_present', 'vineeta_present', 'peyush_present', 'ghazal_present']\n","\n","# Dictionary to store participation rates while taking presence into account\n","adjusted_participation_rates = {}\n","\n","# As the shark deals and presence columns line up exactly, we can zip them together to perform the following iteration\n","for deal, present in zip(sharks, shark_presence):\n","    present_pitches = df[df[present] == 1]\n","    participation_rate = present_pitches[deal].mean() * 100\n","    adjusted_participation_rates[deal.split('_')[0].capitalize()] = f'{participation_rate:.2f}%'\n","\n","adjusted_participation_rates"],"metadata":{"id":"Ej_kmuqYEpiO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After accounting for shark presence, we can see that the participation rates are far more even across the board. Peyush and Aman are the sharks with the highest participation, while Anupam and Namita are the sharks with the lowest."],"metadata":{"id":"GYvq4xneHEMM"}}]}